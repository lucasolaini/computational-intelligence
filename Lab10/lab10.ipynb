{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractclassmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('Position', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player class\n",
    "- Random player: plays always randomly (i.e. choses an action from the set of available actions).\n",
    "- RL player: uses an $\\epsilon$-greedy strategy. This means that it takes a random action with probability $\\epsilon$, the best action (according to the values in the Q-table) with probability 1-$\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe_Player(ABC):\n",
    "    @abstractclassmethod\n",
    "    def next_action(self, current_state):\n",
    "        pass\n",
    "\n",
    "class Random_Player(TicTacToe_Player):\n",
    "    def next_action(self, current_state):\n",
    "        available_moves = set(range(1, 10)) - current_state.x - current_state.o\n",
    "        return choice(list(available_moves))\n",
    "    \n",
    "class RL_Player(TicTacToe_Player):\n",
    "    def __init__(self, epsilon, Q=None) -> None:\n",
    "        if not Q:\n",
    "            self.Q = defaultdict(float)\n",
    "        else:\n",
    "            self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def next_action(self, current_state):\n",
    "        available_moves = list(set(range(1, 10)) - current_state.x - current_state.o)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return choice(available_moves)\n",
    "        else:\n",
    "            # generate action a as the best action we can take in state s\n",
    "            s = (frozenset(current_state.x), frozenset(current_state.o))\n",
    "            q_star_index = np.argmax([self.Q[(s, a)] for a in available_moves])\n",
    "            return available_moves[q_star_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self) -> None:\n",
    "        self.MAGIC = [2, 7, 6,\n",
    "                      9, 5, 1,\n",
    "                      4, 3, 8]\n",
    "\n",
    "    def win(self, elements):\n",
    "        \"\"\"Checks if elements is winning\"\"\"\n",
    "        return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "    \n",
    "    def state_action_value(self, state_action) -> int:\n",
    "        \"\"\"Evaluate position: +1 first player wins, -1 first player loses, 0 game not over\"\"\"\n",
    "\n",
    "        state, action = state_action\n",
    "        \n",
    "        if len(state.x) > len(state.o):\n",
    "            state.o.add(action)\n",
    "        else:   \n",
    "            state.x.add(action)\n",
    "        \n",
    "        if self.win(state.x):\n",
    "            return 1\n",
    "        elif self.win(state.o):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def print_board(self, pos):\n",
    "        \"\"\"Print the board\"\"\"\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                i = r * 3 + c\n",
    "                if self.MAGIC[i] in pos.x:\n",
    "                    print('X', end='')\n",
    "                elif self.MAGIC[i] in pos.o:\n",
    "                    print('O', end='')\n",
    "                else:\n",
    "                    print('.', end='')\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    def play(self, player1: TicTacToe_Player, player2: TicTacToe_Player, verbose=False):\n",
    "        trajectory = list()\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1, 10)) - state.x - state.o\n",
    "        \n",
    "        while available:\n",
    "            x = player1.next_action(state)\n",
    "            trajectory.append((deepcopy(state), x))\n",
    "            state.x.add(x)\n",
    "            available.remove(x)\n",
    "            if verbose:\n",
    "                self.print_board(state)\n",
    "            if self.win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = player2.next_action(state)\n",
    "            trajectory.append((deepcopy(state), o))\n",
    "            state.o.add(o)\n",
    "            available.remove(o)\n",
    "            if verbose:\n",
    "                self.print_board(state)\n",
    "            if self.win(state.o) or not available:\n",
    "                break\n",
    "\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X..\n",
      "...\n",
      "...\n",
      "\n",
      "X..\n",
      "O..\n",
      "...\n",
      "\n",
      "X..\n",
      "O..\n",
      ".X.\n",
      "\n",
      "XO.\n",
      "O..\n",
      ".X.\n",
      "\n",
      "XO.\n",
      "O..\n",
      ".XX\n",
      "\n",
      "XO.\n",
      "O.O\n",
      ".XX\n",
      "\n",
      "XOX\n",
      "O.O\n",
      ".XX\n",
      "\n",
      "XOX\n",
      "OOO\n",
      ".XX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "player1 = Random_Player()\n",
    "player2 = Random_Player()\n",
    "trajectory = game.play(player1, player2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the agent\n",
    "The agent is trained through model-free Q-learning, i.e., using the formula:\n",
    "$$\n",
    "Q^*_{t+1}(s,a) = (1-\\alpha)Q^*_{t}(s,a) + \\alpha(r + \\gamma Q^*_{t}(s',a'))\n",
    "$$\n",
    "\n",
    "Representation adopted:\n",
    "- state: tuple in the form ({positions of X}, {positions of O}) \n",
    "- action: int representing the next position occupied\n",
    "- state/action: tuple in the form (state, action)\n",
    "\n",
    "Thus, the Q-table contains the values associated to each state/action pair. The training is made over 2.000.000 episodes: 1.000.000 episodes playing the agent as first, the remaing playing the agent as second.\n",
    "\n",
    "Note:\n",
    "- s = state in which the agent is\n",
    "- a = action executed by the agent\n",
    "- s' = next state in which the agent is has to move again\n",
    "- a' = best action to take in s', according to Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.01\n",
    "ALPHA = 0.9\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EPISODES = 2_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [05:04<00:00, 6564.83it/s]\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "\n",
    "player1 = Random_Player()\n",
    "player2 = RL_Player(epsilon=EPSILON)\n",
    "\n",
    "Q = player2.Q\n",
    "starting_player = 0\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    # switch the role of the players\n",
    "    player1, player2 = player2, player1\n",
    "\n",
    "    # store all the states/actions taken in the game\n",
    "    trajectory = game.play(player1, player2)\n",
    "\n",
    "    # verify who won\n",
    "    final_reward = game.state_action_value(trajectory[-1])\n",
    "\n",
    "    # switch the role of the players (needed for Q table update)\n",
    "    starting_player = 1 - starting_player\n",
    "\n",
    "    # the reward must be inverted when the agent starts second (i.e. if player 1 loses -> agent wins)\n",
    "    if starting_player == 1:\n",
    "        final_reward = -final_reward\n",
    "    \n",
    "    # for every state/action taken by the agent ([start::2]) execept for the terminal state ([start:-1])\n",
    "    for state, action in trajectory[starting_player:-1:2]:\n",
    "        available_moves = list(set(range(1, 10)) - state.x - state.o)\n",
    "\n",
    "        s = (frozenset(state.x), frozenset(state.o))\n",
    "        a = action\n",
    "\n",
    "        # generate state s'\n",
    "        if len(state.x) < len(state.o):\n",
    "            state.x.add(a)\n",
    "        else:\n",
    "            state.o.add(a)\n",
    "        s1 = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        # generate action a' as the best action we can take in state s'\n",
    "        q_star_index = np.argmax([Q[(s1, action)] for action in available_moves])\n",
    "        a1 = available_moves[q_star_index]\n",
    "\n",
    "        # update the Q table using the formula\n",
    "        Q[(s, a)] = (1 - ALPHA) * Q[(s, a)] +\\\n",
    "                           ALPHA * (final_reward + DISCOUNT_FACTOR * Q[(s1, a1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best states/actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(((frozenset({1, 3, 4}), frozenset({2, 8})), 6), 1.728),\n",
       " (((frozenset({2, 8}), frozenset({5})), 1), 1.7182790837090822),\n",
       " (((frozenset({2, 3, 9}), frozenset({6, 8})), 4), 1.7127990000000002),\n",
       " (((frozenset({3, 9}), frozenset({8})), 6), 1.7000001816216224),\n",
       " (((frozenset({2, 6, 9}), frozenset({3, 7})), 1), 1.700000000991009),\n",
       " (((frozenset({6, 7}), frozenset({8})), 4), 1.7),\n",
       " (((frozenset({1, 3, 9}), frozenset({2, 5})), 7), 1.6470000000000002),\n",
       " (((frozenset({1, 6}), frozenset({8})), 3), 1.6469375899065894),\n",
       " (((frozenset({5, 7, 9}), frozenset({3, 8})), 1), 1.6380000000000003),\n",
       " (((frozenset({1, 7}), frozenset({2})), 4), 1.6298917731798381)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best states/actions\")\n",
    "sorted(Q.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst states/actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(((frozenset({1}), frozenset({8})), 6), -1.0),\n",
       " (((frozenset({1}), frozenset({8})), 7), -1.0),\n",
       " (((frozenset({1, 2}), frozenset({3, 8})), 5), -1.0),\n",
       " (((frozenset({1, 2}), frozenset({3, 8})), 6), -1.0),\n",
       " (((frozenset({1, 2}), frozenset({3, 8})), 9), -1.0),\n",
       " (((frozenset({1, 2, 4, 5}), frozenset({3, 8, 9})), 7), -1.0),\n",
       " (((frozenset({2, 6}), frozenset({1})), 5), -1.0),\n",
       " (((frozenset({2, 5, 6}), frozenset({1, 3})), 8), -1.0),\n",
       " (((frozenset({2, 5, 6}), frozenset({1, 3})), 9), -1.0),\n",
       " (((frozenset({2, 5, 6}), frozenset({1, 3})), 4), -1.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Worst states/actions\")\n",
    "sorted(Q.items(), key=lambda e: e[1], reverse=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent:\n",
    " - when starting first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 games:\n",
      "Wins: 938\t Draws: 62\t Losses: 0\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 1_000\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "# epsilon=0: always take the best action according to the Q table\n",
    "player1 = RL_Player(Q=Q, epsilon=0)\n",
    "player2 = Random_Player()\n",
    "\n",
    "for i in range(N_GAMES):\n",
    "    trajectory = game.play(player1, player2)\n",
    "    final_reward = game.state_action_value(trajectory[-1])\n",
    "\n",
    "    if final_reward == 1:\n",
    "        wins += 1\n",
    "    elif final_reward == -1:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"Results after {N_GAMES} games:\")\n",
    "print(f\"Wins: {wins}\\t Draws: {N_GAMES-wins-losses}\\t Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when starting second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 games:\n",
      "Wins: 670\t Draws: 216\t Losses: 114\n"
     ]
    }
   ],
   "source": [
    "N_GAMES = 1_000\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "\n",
    "# epsilon = 0: always take the best action according to the Q table\n",
    "player1 = Random_Player()\n",
    "player2 = RL_Player(Q=Q, epsilon=0)\n",
    "\n",
    "for i in range(N_GAMES):\n",
    "    trajectory = game.play(player1, player2)\n",
    "    final_reward = game.state_action_value(trajectory[-1])\n",
    "\n",
    "    if final_reward == 1:\n",
    "        losses += 1\n",
    "    elif final_reward == -1:\n",
    "        wins += 1\n",
    "\n",
    "print(f\"Results after {N_GAMES} games:\")\n",
    "print(f\"Wins: {wins}\\t Draws: {N_GAMES-wins-losses}\\t Losses: {losses}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
